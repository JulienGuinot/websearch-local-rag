# RAG local + Websearch

### Skepticism

Service RAG avanc√© avec recherche web et ajout de documents.
Le contenu est transform√© en sa repr√©sentation s√©mantique vectorielle (embeddings), puis stock√© dans une matrice (VectorStore).
On compare ensuite l'embedding de la requete avec la matrice pour identifier les contenus les plus pertinents,
et ainsi enrichir la requ√™te.\
Le projet n'a pas pour vocation de remplacer des LLM plus pouss√©s et utilis√©s (Perplexity, Anthropic, Mistral etc), mais plutot de comprendre comment les repr√©sentations s√©mantiques fonctionnent, et d'obtenir un assistant hors-ligne capable de lire des fichiers.

La comparaison se fait par defaut en utilisant le produit scalaire, soit :

```typescript
export function dotProduct(vec1: number[], vec2: number[]): number {
  return vec1.reduce((sum, val, i) => sum + val * vec2[i], 0);
}
```

Elle peut aussi se faire par similarit√© cosine ou euclidienne.\
Cela est configurable depuis `config/config.ts`

## Vector store

Les embeddings sont enregistr√©s dans le VectorStore (m√©moire). Celui-ci est r√©initialis√© √† la fermeture de programme (store non persistent).
On pourrait utiliser une base de donn√©es sp√©cialis√©e (Pinecone, Chromadb, etc...) mais √† mesure qu'on y ajouterait du contenu,
la r√©cup√©ration de sources par rapport √† une requ√™te pourrait perdre en pertinence. Stocker en m√©moire est un choix personnel pour conserver une pertinence des sources √† chaque utilisation, et offrir une exp√©rience plus satisfaisante.

## Installation

```bash
git clone https://github.com/JulienGuinot/websearch-local-rag
```

ssurez-vous qu'Ollama est install√© et en cours d'ex√©cution :

```bash
# Installer Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# D√©marrer le service
ollama serve

# T√©l√©charger les mod√®les n√©cessaires
ollama pull qwen2.5:0.5b
ollama pull nomic-embed-text
```

Puis

```bash
npm install
npm run dev
```

## Configuration

La configuration du rag se fait dans le fichier config/config.ts

```typescript
export const config: BaseConfig = {
    ollama: {
        baseUrl: 'http://localhost:11434',
        model:  'qwen2.5:0.5b',
        embeddingModel: 'nomic-embed-text',
        temperature: 0.7,
        maxTokens: 2048
    },
    vectorStore: {
        dimensions: 768,
        similarity: 'dot'
    },
    chunking: {
        maxChunkSize: 500,
        overlap: 100
    },
    retrieval: {
        topK: 5,
        threshold: 0.7
    },
    webSearch: {
        searchEngine: "duckduckgo",
        maxResults: 10,
        timeout: 15000,
        retryAttempts: 1,
        retryDelay: 1000,
        minContentLength: 200,
        excludeDomains: ["youtube.com"],
        includeDomains: [],
        userAgent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
```

## Utilisation

### CLI Interactif

```bash
#Pour acc√®s global (depuis n'importe quel path)
npm link
skepticism

#Acc√®s local : confin√© au dossier contenant le code
npm run cli
```

### Commandes disponibles

- `search <query>` - Recherche avec analyse automatique et enrichissement web si n√©cessaire
- `add-web <query>` - Ajouter du contenu depuis le web avec analyse intelligente
- `add-file <path>` - Ajouter un fichier texte √† la base
- `add-folder <path>` - Ajouter les contenus d'un dossier √† la base
- `file:<file>` - Utiliser un fichier de la base comme r√©f√©rence
- `folder:<path>` - Utiliser les contenus d'un dossier comme r√©f√©rence
- `stats` - Afficher les statistiques de la base
- `clear` - Vider la base de connaissances
- `help` - Afficher l'aide
- `exit` - Quitter

### Serveur web

```bash
npm run dev #D√©veloppement
npm run build
npm start #Version build
```

## Exemples d'utilisation CLI

### Exemple avec un fichier

```
Skepticism> add-file smartcontract.rs
‚úì Embeddings g√©n√©r√©s pour 29 chunks en 0.55s
Skepticism> que fais le smartcontract
Analys√© smartcontract.rs
‚†è Recherche dans la base existante...
 Recherche RAG termin√©e en 4366ms avec 15 sources
‚úì Recherche termin√©e!

‚îå‚îÄ R√âPONSE‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îÇ
‚îÇ - D√âFINITION DU SMART CONTRACT
‚îÇ ==============================
‚îÇ
‚îÇ Le smart contract est une application logicielle qui ex√©cute des
‚îÇ instructions de mani√®re d√©centralis√©e et s√©curis√©e. Dans ce cas, le smart
‚îÇ contract est utilis√© pour g√©rer les flux de pr√™t (flash-loan) entre deux
‚îÇ programmes : Orca et Raydium.
‚îÇ
‚îÇ
‚îÇ ‚ñ∂ FLUX DE PR√äT
‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îÇ
‚îÇ Le processus de pr√™t fonctionne comme suit :
‚îÇ 1.  Pr√©paration du contexte : Le client cr√©e un contexte de flash-loan en
‚îÇ fournissant des informations sur le programme √† utiliser (Orca ou Raydium),
‚îÇ la quantit√© d'argent √† emprunter, les param√®tres de classement et les
‚îÇ conditions de paiement.
‚îÇ 2.  Ex√©cution du pr√™t : Le smart contract ex√©cute le pr√™t en utilisant les
‚îÇ informations fournies dans l'√©tape pr√©c√©dente.
‚îÇ
‚îÇ
‚îÇ ‚ñ∂ EX√âCUTION DU SMART CONTRACT
‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îÇ    Le smart contract v√©rifie les conditions d'ex√©cution (par exemple, si le
‚îÇ programme est autoris√© √† √™tre utilis√©) avant de proc√©der.
‚îÇ    Il utilise des instructions cpi_proxy_invoke pour appeler les functions du
‚îÇ programme cible (Orca ou Raydium).
‚îÇ ‚Ä¢   Il traite les gains et les pertes dans le cas d'une transaction
‚îÇ r√©ussie.
‚îÇ
‚îÇ En r√©sum√©, le smart contract est une solution s√©curis√©e pour g√©rer les flux
‚îÇ de pr√™t entre des programmes d√©centralis√©s.
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

üìö Sources:
  1 smartcontract.rs
```

### Exemple avec un dossier:

```
Skepticism> add-folder dogs
‚è≠Ô∏è  Dossier ignor√©: .git
‚úì Embeddings g√©n√©r√©s pour 7 chunks en 0.37s
‚úì Embeddings g√©n√©r√©s pour 1 chunks en 0.02s
‚úì Embeddings g√©n√©r√©s pour 5 chunks en 0.06s
‚úì Embeddings g√©n√©r√©s pour 5 chunks en 0.05s
‚úì Embeddings g√©n√©r√©s pour 1 chunks en 0.02s
‚è≠Ô∏è  Dossier ignor√©: venv
‚è≠Ô∏è  Dossier ignor√©: __pycache__
Skepticism> Que fais ce projet
Analys√© inference.py
Analys√© README.md
Analys√© classifier.py
Analys√© gpus_available.py
‚†ã Recherche dans la base existante...
 Recherche RAG termin√©e en 5601ms avec 15 sources
‚úì Recherche termin√©e!

‚îå‚îÄ R√âPONSE‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îÇ Ce projet est un classificateur de races de chiens bas√© sur le deep
‚îÇ learning. Il utilise une approche de transfer learning avec MobileNetV2
‚îÇ pr√©-entra√Æn√© sur ImageNet pour classifier les images de chiens parmi 120
‚îÇ races du dataset Stanford Dogs. Le projet permet d'inf√©rer la race d'un
‚îÇ chien √† partir d'une image, en pr√©dissant la classe la plus probable de
‚îÇ l'image dans le dataset.
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

üìö Sources:
  1 inference.py
  2 README.md
  3 classifier.py
  4 gpus_available.py
Skepticism>
```

## Limitations du Rag

La transformation du contenu ajout√© en embeddings peut prendre un certain temps. c'est le principal goulot d'√©tranglement de ce syst√®me. On pourrait utiliser un mod√®le plus petit pour g√©n√©rer les embeddings, comme "miniailm", ou passer le texte de la recherche web / document directement, mais on perdrait en qualit√© sur le ranking des chunks, et la r√©ponse finale pourrait √™tre moins pertinente.
Plus le mod√®le d'embedding est petit, plus la dimension des embeddings (nombre de composants d'un vecteur) de sortie est petite :

#### Pour se rendre compte :

```
miniailm -> 384 dimensions soit un vecteur contenant 384 √©l√©ments
nomic-embed-text -> 768 dimensions soit un vecteur comprenant 768 √©l√©ments
```

## Architecture

### Injection de d√©pendences avec Awilix

le RAG est orch√©str√© par la classe `services/rag.service.ts` la classe doit √™tre instanci√©e avec un objet `{di}`, export√© depuis `services/di-container` qui expose les services et g√®re les √©tats, pour √©viter la multi-instanciation des classes et la perte des √©tats

```typescript
export const di = {
  aiService: container.resolve<OllamaService>("aiService"),
  vectorStore: container.resolve<VectorStore>("vectorStore"),
  searchService: container.resolve<SearchService>("searchService"),
  textChunker: container.resolve<TextChunker>("textChunker"),
};
```

puis, on initialise le RagService en lui passant l'objet `{di}`

```typescript
const ragService = new RAGService(di);
```

### Fonction "Factory" performSearch

Permet de changer le moteur de recherche utilis√© par le RAG, en une seule ligne, depuis la config

- A noter que Google ne ne fonctionne jamais durablement (les s√©l√©cteurs html/css changent r√©guli√®rement)

```typescript
export async function performSearch(
  query: string,
  searchEngine: SearchEngine,
  config: WebSearchConfig,
  userAgent: string
): Promise<SearchResult[]> {
  switch (searchEngine) {
    case "duckduckgo":
      return await searchDuckDuckGo(query, config, userAgent);
    case "bing":
      return await searchWithBing(query, config, userAgent);
    case "google":
      return await searchWithGoogle(query, config, userAgent);
    default:
      return await searchDuckDuckGo(query, config, userAgent);
  }
}
```

## Contribution

Toutes les contributions sont les bienvenues !

## Licence

MIT
