# RAG (Retrieval-Augmented Generation) local 

Service RAG  avancÃ© avec recherche web et ajout de documents.
Le contenu est transformÃ© en sa reprÃ©sentation sÃ©mantique vectorielle (embeddings), puis stockÃ© dans une matrice (VectorStore).
On compare ensuite l'embedding de la requete avec la matrice pour identifier les contenus les plus pertinents,
et ainsi enrichir la requÃªte. 

La comparaison se fait par defaut en utilisant la similaritÃ© cosine, soit :
```Latex
similaritÃ© = sin(Angle entre les deux vecteurs)
--> retourne un score de similaritÃ© compris entre 0 et 1
```
Elle peut aussi se faire par similaritÃ© euclidienne ou par produit scalaire.
Cela est configurable depuis `config/config.ts`

## Vector store
Les emebeddings sont enregistrÃ©s dans le vectorStore (mÃ©moire). Celui-ci est rÃ©initialisÃ© Ã  la fermeture de programme (store non persistent).



## Installation

```bash
git clone https://github.com/JulienGuinot/Skepticism
```

ssurez-vous qu'Ollama est installÃ© et en cours d'exÃ©cution :

```bash
# Installer Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# DÃ©marrer le service
ollama serve

# TÃ©lÃ©charger les modÃ¨les nÃ©cessaires
ollama pull qwen2.5:0.5b
ollama pull nomic-embed-text
```


Puis 

```bash
npm install
npm run dev
```

## Configuration

La configuration du rag se fait dans le fichier config/config.ts

```typescript
export const config: BaseConfig = {
    ollama: {
        baseUrl: 'http://localhost:11434',
        model: process.env.MODEL || 'qwen2.5:0.5b',
        embeddingModel: 'nomic-embed-text',
        temperature: 0.7,
        maxTokens: 2048
    },
    vectorStore: {
        dimensions: 768,
        similarity: 'cosine'
    },
    chunking: {
        maxChunkSize: 500,
        overlap: 100
    },
    retrieval: {
        topK: 5,
        threshold: 0.7
    },
    webSearch: {
        searchEngine: "duckduckgo",
        maxResults: 10,
        timeout: 15000,
        retryAttempts: 1,
        retryDelay: 1000,
        minContentLength: 200,
        excludeDomains: ["youtube.com"],
        includeDomains: [],
        userAgent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
```

## Utilisation

### CLI Interactif

```bash
npm run cli
```

### Commandes disponibles

- `search <query>` - Recherche avec analyse automatique et enrichissement web si nÃ©cessaire
- `add-web <query>` - Ajouter du contenu depuis le web avec analyse intelligente
- `add-file <path>` - Ajouter un fichier texte Ã  la base
- `stats` - Afficher les statistiques de la base
- `clear` - Vider la base de connaissances
- `help` - Afficher l'aide
- `exit` - Quitter


### Serveur web
```bash
npm run dev #DÃ©veloppement 
npm run build 
npm start #Version build  
```


### Exemples d'utilisation

```
Skepticism> Comment fonctionne le machine learning avec des rÃ©seaux de neurones
ğŸ” Recherche intelligente: "comment fonctionne le machine learning avec des rÃ©seaux de neurones"
âœ“ Recherche dans la base existante...
âœ“ Base existante insuffisante, recherche web en cours...

ğŸ“Š Analyse automatique:
  Sujets identifiÃ©s: machine, learning, rÃ©seaux, neurones
  Stop words supprimÃ©s: comment, fonctionne, le, avec, des, de
  RequÃªte optimisÃ©e: "machine learning rÃ©seaux neurones"

âœ“ 8 nouveaux documents ajoutÃ©s
  Variantes utilisÃ©es: machine learning rÃ©seaux neurones | machine learning | rÃ©seaux neurones

âœ“ Recherche terminÃ©e!

â”Œâ”€ RÃ‰PONSEâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ Le machine learning avec des rÃ©seaux de neurones fonctionne en...
â”‚ [RÃ©ponse dÃ©taillÃ©e basÃ©e sur le contenu enrichi]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“š Sources:
  1 https://example.com/neural-networks-guide
  2 https://example.com/ml-fundamentals
```



## Limitations du Rag


La transformation du contenu ajoutÃ© en embeddings peut prendre un certain temps. c'est le principal goulot d'Ã©tranglement de ce systÃ¨me. On pourrait utiliser un odÃ¨le plus petit pour gÃ©nÃ©rer les embeddings, comme "miniailm", ou passer le texte de la recherche web / document directement, mais on perdrait le ranking des chunks, et la rÃ©ponse finale pourrait Ãªtre moins pertinente



## Architecture 


### Injection de dÃ©pendences avec Awilix
le RAG est orchÃ©strÃ© par la classe `services/rag.service.ts` la classe doit Ãªtre instanciÃ©e avec un objet `{di}`, exportÃ© depuis `services/di-container` qui expose les services et gÃ¨re les Ã©tats, pour Ã©viter la multi-instanciation des classes et la perte des Ã©tats

```typescript
export const di = {
    aiService: container.resolve<OllamaService>("aiService"),
    vectorStore: container.resolve<VectorStore>("vectorStore"),
    searchService: container.resolve<SearchService>("searchService"),
    textChunker: container.resolve<TextChunker>("textChunker")
}
```


puis, on initialise le RagService en lui passant l'objet `{di}`
```typescript
const ragService = new RAGService(di);
```
### Fonction "Factory" performSearch
Permet de changer le moteur de recherche utilisÃ© par le RAG, en une seule ligne, depuis la config
```typescript
export async function performSearch(
    query: string,
    searchEngine: SearchEngine,
    config: WebSearchConfig,
    userAgent: string
): Promise<SearchResult[]> {

    switch (searchEngine) {
        case 'duckduckgo':
            return await searchDuckDuckGo(query, config, userAgent);
        case "bing":
            return await searchWithBing(query, config, userAgent)
        case "google":
            return await searchWithGoogle(query, config, userAgent)
        default:
            return await searchDuckDuckGo(query, config, userAgent);
    }
}
```




## Contribution 
Toutes les contributions sont les bienvenues !


## Licence
MIT